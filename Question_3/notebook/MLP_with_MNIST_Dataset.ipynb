{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MLP with MNIST Dataset with Triplet Loss"
      ],
      "metadata": {
        "id": "T2QtC2TmcIE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Library"
      ],
      "metadata": {
        "id": "OlQQOg_aocWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "UWQpnfm8obmr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load MNIST Dataset"
      ],
      "metadata": {
        "id": "jGj5v2Pjod4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SIvrY8xBk6xK"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
        "X = mnist['data'].values\n",
        "y = mnist['target'].astype(np.int32).values\n",
        "\n",
        "# Normalize the data\n",
        "X = X / 255.0\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Show Image"
      ],
      "metadata": {
        "id": "FkorCkF3B9u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image = X_test[0].reshape((28, 28))\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "my2-9WMDA-Ns",
        "outputId": "1fccbb21-011e-421c-88e9-fe67cc31d519"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqd0NVeVBCJD",
        "outputId": "c31daa89-a21f-4da7-8ae7-51ea9e7f3d78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. MLP Model"
      ],
      "metadata": {
        "id": "sQKfzPaeogmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Utils"
      ],
      "metadata": {
        "id": "_MAp8ZWPohm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
        "    \"\"\"\n",
        "    Computes the Triplet Loss for multiple anchors, positives, and negatives.\n",
        "\n",
        "    Parameters:\n",
        "    - anchor: np.ndarray, feature vector of the anchors.\n",
        "    - positive: np.ndarray, feature vector of the positive.\n",
        "    - negative: np.ndarray, feature vector of the negative.\n",
        "    - margin: float, margin for calculating the loss.\n",
        "\n",
        "    Returns:\n",
        "    - total_loss: float, the value of the triplet loss.\n",
        "    \"\"\"\n",
        "    # Compute the squared distance between the anchor and the positive example\n",
        "    pos_dist = np.sum(np.square(anchor - positive), axis=-1)\n",
        "\n",
        "    # Compute the squared distance between the anchor and the negative example\n",
        "    neg_dist = np.sum(np.square(anchor - negative), axis=-1)\n",
        "\n",
        "    # Compute the Triplet Loss\n",
        "    loss = np.maximum(0, pos_dist - neg_dist + margin)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def load_data():\n",
        "    # Load MNIST dataset\n",
        "    mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
        "    X = mnist['data'].values\n",
        "    y = mnist['target'].astype(np.int32).values\n",
        "\n",
        "    # Normalize the data\n",
        "    X = X / 255.0\n",
        "\n",
        "    # Binarize the labels\n",
        "    y = (y == 0).astype(np.int32)\n",
        "\n",
        "    # Split into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Ensure the data is in the right shape for MLP (N, D) where N is number of samples, D is number of features\n",
        "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def create_triplets(X, y, batch_size):\n",
        "    \"\"\"\n",
        "    Create triplets (anchor, positive, negative) for training.\n",
        "\n",
        "    Parameters:\n",
        "    - X: np.ndarray, feature vectors of the samples\n",
        "    - y: np.ndarray, labels of the samples\n",
        "    - batch_size: int, number of triplets in the batch\n",
        "\n",
        "    Returns:\n",
        "    - anchor: np.ndarray, anchor samples\n",
        "    - positive: np.ndarray, positive samples\n",
        "    - negative: np.ndarray, negative samples\n",
        "    \"\"\"\n",
        "    anchor, positive, negative = [], [], []\n",
        "    for _ in range(batch_size):\n",
        "        # Select anchor sample\n",
        "        idx = np.random.randint(0, len(X))\n",
        "        anchor.append(X[idx])\n",
        "\n",
        "        # Select positive sample (different sample of the same class)\n",
        "        pos_idxs = np.where(y == y[idx])[0]\n",
        "        pos_idx = np.random.choice(pos_idxs[pos_idxs != idx])\n",
        "        positive.append(X[pos_idx])\n",
        "\n",
        "        # Select negative sample (sample of a different class)\n",
        "        neg_idxs = np.where(y != y[idx])[0]\n",
        "        neg_idx = np.random.choice(neg_idxs)\n",
        "        negative.append(X[neg_idx])\n",
        "\n",
        "    return np.array(anchor), np.array(positive), np.array(negative)\n",
        "\n",
        "def extract_label_features(model, X_train, y_train):\n",
        "    # Extract one sample for each label\n",
        "    unique_labels = np.unique(y_train)\n",
        "    label_samples = {label: X_train[np.where(y_train == label)[0][0]] for label in unique_labels}\n",
        "\n",
        "    # Extract features for each label sample\n",
        "    label_features = {label: model.forward(sample) for label, sample in label_samples.items()}\n",
        "\n",
        "    # Store the features in a list\n",
        "    label_features_list = [label_features[label] for label in unique_labels]\n",
        "\n",
        "    return label_features_list, unique_labels\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    \"\"\"\n",
        "    Compute the cosine similarity between two vectors.\n",
        "\n",
        "    Parameters:\n",
        "    - v1: np.ndarray, first vector\n",
        "    - v2: np.ndarray, second vector\n",
        "\n",
        "    Returns:\n",
        "    - similarity: float, cosine similarity score\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(v1, v2.T)\n",
        "    norm_v1 = np.linalg.norm(v1)\n",
        "    norm_v2 = np.linalg.norm(v2)\n",
        "    similarity = dot_product / (norm_v1 * norm_v2)\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "LDzBzsX0mW2l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Architecture"
      ],
      "metadata": {
        "id": "M3gl2JMcojU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        return self.z2\n",
        "\n",
        "    def compute_loss(self, anchor, positive, negative, alpha=0.2):\n",
        "        anchor_output = self.forward(anchor)\n",
        "        positive_output = self.forward(positive)\n",
        "        negative_output = self.forward(negative)\n",
        "\n",
        "        # Ensure there are no NaN values in outputs\n",
        "        if np.isnan(anchor_output).any() or np.isnan(positive_output).any() or np.isnan(negative_output).any():\n",
        "            print(\"NaN detected in forward pass outputs\")\n",
        "\n",
        "        loss = triplet_loss(anchor_output, positive_output, negative_output, alpha)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, anchor, positive, negative, alpha=0.2, learning_rate=0.01):\n",
        "        # Forward pass\n",
        "        anchor_output = self.forward(anchor)\n",
        "        positive_output = self.forward(positive)\n",
        "        negative_output = self.forward(negative)\n",
        "\n",
        "        # Calculate gradients (simple backpropagation)\n",
        "        pos_dist = 2 * (anchor_output - positive_output)\n",
        "        neg_dist = 2 * (anchor_output - negative_output)\n",
        "\n",
        "        dloss_da = pos_dist - neg_dist\n",
        "        dloss_dp = -pos_dist\n",
        "        dloss_dn = neg_dist\n",
        "\n",
        "        # Ensure there are no NaN values in gradients\n",
        "        if np.isnan(dloss_da).any() or np.isnan(dloss_dp).any() or np.isnan(dloss_dn).any():\n",
        "            print(\"NaN detected in gradients\")\n",
        "\n",
        "        # Update weights and biases (simplified gradient descent)\n",
        "        self.W2 -= learning_rate * np.dot(self.a1.T, dloss_da)\n",
        "        self.b2 -= learning_rate * np.sum(dloss_da, axis=0, keepdims=True)\n",
        "\n",
        "        dW1_a = np.dot(anchor.T, np.dot(dloss_da, self.W2.T) * (self.z1 > 0))\n",
        "        db1_a = np.sum(np.dot(dloss_da, self.W2.T) * (self.z1 > 0), axis=0, keepdims=True)\n",
        "\n",
        "        dW1_p = np.dot(positive.T, np.dot(dloss_dp, self.W2.T) * (self.z1 > 0))\n",
        "        db1_p = np.sum(np.dot(dloss_dp, self.W2.T) * (self.z1 > 0), axis=0, keepdims=True)\n",
        "\n",
        "        dW1_n = np.dot(negative.T, np.dot(dloss_dn, self.W2.T) * (self.z1 > 0))\n",
        "        db1_n = np.sum(np.dot(dloss_dn, self.W2.T) * (self.z1 > 0), axis=0, keepdims=True)\n",
        "\n",
        "        self.W1 -= learning_rate * (dW1_a + dW1_p + dW1_n)\n",
        "        self.b1 -= learning_rate * (db1_a + db1_p + db1_n)"
      ],
      "metadata": {
        "id": "xahwO4Ink_lT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train"
      ],
      "metadata": {
        "id": "WiTV68zeolXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28 * 28\n",
        "hidden_size = 128\n",
        "output_size = 64\n",
        "model = MLP(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "_ll56XFAnpFl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "8U3DGQzWnk-o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    num_batches = len(X_train) // batch_size\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        # Create triplet batches\n",
        "        anchor, positive, negative = create_triplets(X_train, y_train, batch_size)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = model.compute_loss(anchor, positive, negative)\n",
        "        epoch_loss += np.sum(loss)\n",
        "\n",
        "        # Backward pass and update weights\n",
        "        model.backward(anchor, positive, negative, learning_rate)\n",
        "\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGTJkYdZniJV",
        "outputId": "26fedd3f-b1fc-43d2-86ed-1f8dff1c73da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 6.3979\n",
            "Epoch 2/10, Loss: 6.4000\n",
            "Epoch 3/10, Loss: 6.4000\n",
            "Epoch 4/10, Loss: 6.4000\n",
            "Epoch 5/10, Loss: 6.4000\n",
            "Epoch 6/10, Loss: 6.4000\n",
            "Epoch 7/10, Loss: 6.4000\n",
            "Epoch 8/10, Loss: 6.4000\n",
            "Epoch 9/10, Loss: 6.4000\n",
            "Epoch 10/10, Loss: 6.4000\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Inference & Evaluate"
      ],
      "metadata": {
        "id": "wY_sfHaAonbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, input):\n",
        "    # Load the data\n",
        "    X_train, X_test, y_train, y_test = load_data()\n",
        "    label_features_list, unique_labels = extract_label_features(model, X_train, y_train)\n",
        "\n",
        "    # Extract features from the input sample\n",
        "    input_features = model.forward(input)\n",
        "\n",
        "    # Calculate similarity scores with each label feature\n",
        "    similarities = [cosine_similarity(input_features, label_feature) for label_feature in label_features_list]\n",
        "\n",
        "    # Find the label with the highest similarity score\n",
        "    predicted_label = unique_labels[np.argmax(similarities)]\n",
        "\n",
        "    return predicted_label"
      ],
      "metadata": {
        "id": "hKmQNMnuWzEt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sample = X_test[0]  # Example input sample from test set\n",
        "predicted_label = inference(model, input_sample)\n",
        "print(f\"Predicted Label: {predicted_label}, True Label: {y_test[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAmtu-sBbvok",
        "outputId": "18cb30aa-cbe5-4e8f-ff57-836346a19d4e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 1, True Label: 8\n"
          ]
        }
      ]
    }
  ]
}