{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "OlQQOg_aocWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "UWQpnfm8obmr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load MNIST Dataset"
      ],
      "metadata": {
        "id": "jGj5v2Pjod4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIvrY8xBk6xK",
        "outputId": "e72920a8-aca1-4abd-f2e1-645162d0ba29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X = mnist['data'].values\n",
        "y = mnist['target'].astype(np.int32).values\n",
        "\n",
        "# Normalize the data\n",
        "X = X / 255.0\n",
        "\n",
        "# Binarize the labels\n",
        "y = (y == 0).astype(np.int32)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure the data is in the right shape for MLP (N, D) where N is number of samples, D is number of features\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle data\n",
        "permutation = np.random.permutation(X_train.shape[0])\n",
        "X_train = X_train[permutation][:1000]\n",
        "y_train = y_train[permutation][:1000]\n",
        "\n",
        "X_test = X_test[:100]\n",
        "y_test = y_test[:100]"
      ],
      "metadata": {
        "id": "uPuUzTGjna_g"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Model"
      ],
      "metadata": {
        "id": "sQKfzPaeogmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triplet Loss"
      ],
      "metadata": {
        "id": "_MAp8ZWPohm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
        "    \"\"\"\n",
        "    Tính toán Triplet Loss.\n",
        "\n",
        "    Parameters:\n",
        "    - anchor: np.ndarray, vector đặc trưng của anchor.\n",
        "    - positive: np.ndarray, vector đặc trưng của positive.\n",
        "    - negative: np.ndarray, vector đặc trưng của negative.\n",
        "    - margin: float, margin để tính toán loss.\n",
        "\n",
        "    Returns:\n",
        "    - loss: float, giá trị của triplet loss.\n",
        "    \"\"\"\n",
        "    # Tính toán khoảng cách bình phương giữa anchor và positive\n",
        "    pos_dist = np.sum(np.square(anchor - positive), axis=-1)\n",
        "\n",
        "    # Tính toán khoảng cách bình phương giữa anchor và negative\n",
        "    neg_dist = np.sum(np.square(anchor - negative), axis=-1)\n",
        "\n",
        "    # Tính toán Triplet Loss\n",
        "    loss = np.maximum(0, pos_dist - neg_dist + margin)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "LDzBzsX0mW2l"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ],
      "metadata": {
        "id": "M3gl2JMcojU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        return self.z2\n",
        "\n",
        "    def compute_loss(self, anchor, positive, negative, alpha=0.2):\n",
        "        anchor_output = self.forward(anchor)\n",
        "        positive_output = self.forward(positive)\n",
        "        negative_output = self.forward(negative)\n",
        "\n",
        "        # Ensure there are no NaN values in outputs\n",
        "        if np.isnan(anchor_output).any() or np.isnan(positive_output).any() or np.isnan(negative_output).any():\n",
        "            print(\"NaN detected in forward pass outputs\")\n",
        "\n",
        "        loss = triplet_loss(anchor_output, positive_output, negative_output, alpha)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, anchor, positive, negative, alpha=0.2, learning_rate=0.01):\n",
        "        # Forward pass\n",
        "        anchor_output = self.forward(anchor)\n",
        "        positive_output = self.forward(positive)\n",
        "        negative_output = self.forward(negative)\n",
        "\n",
        "        # Calculate gradients (simple backpropagation)\n",
        "        pos_dist = 2 * (anchor_output - positive_output)\n",
        "        neg_dist = 2 * (anchor_output - negative_output)\n",
        "\n",
        "        dloss_da = pos_dist - neg_dist\n",
        "        dloss_dp = -pos_dist\n",
        "        dloss_dn = neg_dist\n",
        "\n",
        "        # Ensure there are no NaN values in gradients\n",
        "        if np.isnan(dloss_da).any() or np.isnan(dloss_dp).any() or np.isnan(dloss_dn).any():\n",
        "            print(\"NaN detected in gradients\")\n",
        "\n",
        "        # Update weights and biases (simplified gradient descent)\n",
        "        self.W2 -= learning_rate * np.dot(self.a1.T, dloss_da)\n",
        "        self.b2 -= learning_rate * np.sum(dloss_da, axis=0, keepdims=True)\n",
        "\n",
        "        dW1_a = np.dot(anchor.T, np.dot(dloss_da, self.W2.T) * (self.z1 > 0))\n",
        "        db1_a = np.sum(np.dot(dloss_da, self.W2.T) * (self.z1 > 0), axis=0, keepdims=True)\n",
        "\n",
        "        dW1_p = np.dot(positive.T, np.dot(dloss_dp, self.W2.T) * (self.z1 > 0))\n",
        "        db1_p = np.sum(np.dot(dloss_dp, self.W2.T) * (self.z1 > 0), axis=0, keepdims=True)\n",
        "\n",
        "        dW1_n = np.dot(negative.T, np.dot(dloss_dn, self.W2.T) * (self.z1 > 0))\n",
        "        db1_n = np.sum(np.dot(dloss_dn, self.W2.T) * (self.z1 > 0), axis=0, keepdims=True)\n",
        "\n",
        "        self.W1 -= learning_rate * (dW1_a + dW1_p + dW1_n)\n",
        "        self.b1 -= learning_rate * (db1_a + db1_p + db1_n)"
      ],
      "metadata": {
        "id": "xahwO4Ink_lT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "WiTV68zeolXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28 * 28\n",
        "hidden_size = 128\n",
        "output_size = 64\n",
        "model = MLP(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "_ll56XFAnpFl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "8U3DGQzWnk-o"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "for epoch in range(num_epochs):\n",
        "    LOSS = []\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        # Ensure the batch size is consistent\n",
        "        end = i + batch_size\n",
        "        if end > X_train.shape[0]:\n",
        "            break\n",
        "\n",
        "        anchor_batch = X_train[i:end]\n",
        "        positive_batch = X_train[i:end]\n",
        "        negative_batch = X_train[(i+batch_size) % X_train.shape[0]: (i+2*batch_size) % X_train.shape[0]]\n",
        "\n",
        "        if len(negative_batch) < len(anchor_batch):\n",
        "            continue\n",
        "\n",
        "        loss = model.compute_loss(anchor_batch, positive_batch, negative_batch)\n",
        "        LOSS.append(loss)\n",
        "        model.backward(anchor_batch, positive_batch, negative_batch, learning_rate=learning_rate)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {np.mean(LOSS)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGTJkYdZniJV",
        "outputId": "3771cecf-9d8f-4964-8174-bfc0bd14b244"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.19589026263522016\n",
            "Epoch 2/10, Loss: 0.1958584359238322\n",
            "Epoch 3/10, Loss: 0.1970650692754378\n",
            "Epoch 4/10, Loss: 0.19775026682885705\n",
            "Epoch 5/10, Loss: 0.19811557494455567\n",
            "Epoch 6/10, Loss: 0.19836189140650695\n",
            "Epoch 7/10, Loss: 0.19854594905470904\n",
            "Epoch 8/10, Loss: 0.1986925931264116\n",
            "Epoch 9/10, Loss: 0.19881436013012715\n",
            "Epoch 10/10, Loss: 0.19891933055689073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference & Evaluate"
      ],
      "metadata": {
        "id": "wY_sfHaAonbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_test = X_test[:batch_size]\n",
        "positive_test = X_test[:batch_size]\n",
        "negative_test = X_test[batch_size:2*batch_size]"
      ],
      "metadata": {
        "id": "E7WV8xVvoLGH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = model.compute_loss(anchor_test, positive_test, negative_test)"
      ],
      "metadata": {
        "id": "iA-LADyBoHPF"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}